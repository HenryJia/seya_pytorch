{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Turing Machine with Keras\n",
    "Definitions:\n",
    "$h$ controller RNN state. $M$ memory (a matrix). $r$ and $w$ read and write vectors, output of a softmax, thus a probability distribution, they are used to do weighted averaging (it seems that differentiable attention is the new name for that) over the rows of $M$. The rows of $M$ are the different \"memory locations\".  \n",
    "\n",
    "Internal algorithm:\n",
    "1) Using $h_{t-1}$, update the reading vector $r_t = f_r(r_{t-1}, h_{t-1})$  \n",
    "2) Read from memory $m_t = read(r_t, M_{t-1})$  \n",
    "3) Using the input and the read vector, update the RNN controller state $RNN$: $h_t = RNN(x_t, m_t, h_{t-1})$  \n",
    "4) Using $h_t$, update the writing vector $w_t = f_w(w_{tm1}, h_t)$  \n",
    "5) Write to memory $M_t = write(M_{t-1}, h_t, w_t)$.  \n",
    "For details see Graves et. al.. Next we are going to run the Copy experiment in PyTorch  \n",
    "\n",
    "# Copy Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter, Module, Linear\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Steal the progbar from Keras\n",
    "from seya_pytorch.utils import generic_utils\n",
    "\n",
    "from seya_pytorch.layers.ntm import NTM\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTMModel(Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_size, m_length, n_slots, shift_range = 3):\n",
    "\n",
    "        super(NTMModel, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.m_length = m_length\n",
    "        self.n_slots = n_slots\n",
    "        self.shift_range = shift_range\n",
    "\n",
    "        self.ntm = NTM(input_dim, hidden_size, m_length, n_slots)\n",
    "        self.fc1 = Linear(hidden_size, input_dim)\n",
    "\n",
    "        self.init_states()\n",
    "\n",
    "    def init_states(self):\n",
    "        # Initial memory and head for the NTM\n",
    "        self.memory = Parameter(torch.ones(1, self.n_slots, self.m_length) * 1e-3)\n",
    "        self.read_head = Parameter(Tensor(1, self.n_slots))\n",
    "        self.write_head = Parameter(Tensor(1, self.n_slots))\n",
    "\n",
    "        # Initial states for the LSTM controller of the NTM\n",
    "        self.h = Parameter(torch.zeros(1, self.hidden_size))\n",
    "        self.c = Parameter(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "        # Since the initial read and write heads are basically weights we'll initialise them like weights\n",
    "        bound = np.sqrt(6.0 / (1 + self.n_slots))\n",
    "        self.read_head.data.uniform_(bound, -bound)\n",
    "        self.write_head.data.uniform_(bound, -bound)\n",
    "\n",
    "    def get_init_states(self, batch_size):\n",
    "        memory = self.memory.expand(batch_size, self.n_slots, self.m_length)\n",
    "        read_head = F.softmax(self.read_head.expand(batch_size, self.n_slots))\n",
    "        write_head = F.softmax(self.write_head.expand(batch_size, self.n_slots))\n",
    "\n",
    "        h = self.h.expand(batch_size, self.hidden_size)\n",
    "        c = self.c.expand(batch_size, self.hidden_size)\n",
    "\n",
    "        return [h, c], [read_head, write_head], memory\n",
    "\n",
    "    def forward(self, inp, states, heads, memory):\n",
    "\n",
    "        ntm_out = self.ntm(inp, states, heads, memory)\n",
    "        out = F.sigmoid(self.fc1(ntm_out[0][0]))\n",
    "\n",
    "        return out, ntm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steal this from EderSanta/Seya\n",
    "def get_sample(batch_size=128, n_bits=8, max_size=20, min_size=1):\n",
    "    # generate samples with random length\n",
    "    inp = np.zeros((batch_size, 2*max_size-1, n_bits))\n",
    "    out = np.zeros((batch_size, 2*max_size-1, n_bits))\n",
    "    sw = np.zeros((batch_size, 2*max_size-1, 1))\n",
    "    for i in range(batch_size):\n",
    "        t = np.random.randint(low=min_size, high=max_size)\n",
    "        x = np.random.uniform(size=(t, n_bits)) > .5\n",
    "        for j,f in enumerate(x.sum(axis=-1)): # remove fake flags\n",
    "            if f>=n_bits:\n",
    "                x[j, :] = 0.\n",
    "        del_flag = np.ones((1, n_bits))\n",
    "        inp[i, :t+1] = np.concatenate([x, del_flag], axis=0)\n",
    "        out[i, t:(2*t)] = x\n",
    "        sw[i, t:(2*t)] = 1\n",
    "    return inp, out, sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steal this from EderSanta/Seya too\n",
    "def show_pattern(inp, out, sw, file_name='pattern2.png'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(inp>.5)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(out>.5)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(sw[:, :1]>.5)\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "inp, out, sw = get_sample()\n",
    "show_pattern(inp[0], out[0], sw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c3cf46358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAEICAYAAAANwHx+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI4NUkhoZpJLUyCCVpEb/HytVmCx9szl1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c3f4acbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Steal this from EderSanta/Seya\n",
    "inp, out, sw = get_sample(1, 8, 20)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('input')\n",
    "plt.imshow(inp[0], cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.title('desired')\n",
    "plt.imshow(out[0], cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.title('sample_weight')\n",
    "plt.imshow(sw[0], cmap='gray')\n",
    "\n",
    "# sample_weight marks the points in time that will \n",
    "# be part of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(self, input_dim, hidden_size, m_length, n_slots, shift_range = 3)\n",
    "model = NTMModel(8, 64, 20, 50)\n",
    "model.cuda()\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/4000 [==>...........................] - ETA: 2136s - loss: 0.1650\n",
      "accuracy  0.82578125\n",
      "1000/4000 [======>.......................] - ETA: 1763s - loss: 0.1575\n",
      "accuracy  0.85888671875\n",
      "1500/4000 [==========>...................] - ETA: 1443s - loss: 0.1514\n",
      "accuracy  0.820629882813\n",
      "2000/4000 [==============>...............] - ETA: 1144s - loss: 0.1458\n",
      "accuracy  0.835131835937\n",
      "2500/4000 [=================>............] - ETA: 853s - loss: 0.1407\n",
      "accuracy  0.85234375\n",
      "2978/4000 [=====================>........] - ETA: 589s - loss: 0.1377"
     ]
    }
   ],
   "source": [
    "nb_epoch = 4000\n",
    "batch_size = 256\n",
    "progbar = generic_utils.Progbar(nb_epoch)\n",
    "for e in range(nb_epoch):\n",
    "    \n",
    "    data, targets, weights = get_sample(n_bits=8, max_size=20, min_size=1, batch_size=batch_size)\n",
    "\n",
    "    x = Variable(torch.from_numpy(np.transpose(data, (1, 0, 2))).float()).cuda()\n",
    "    t = Variable(torch.from_numpy(np.transpose(targets, (1, 0, 2))).float()).cuda()\n",
    "    w = Variable(torch.from_numpy(np.transpose(weights, (1, 0, 2))).float()).cuda()\n",
    "\n",
    "    model.zero_grad()\n",
    "    out = []\n",
    "    states = model.get_init_states(batch_size)\n",
    "    x = x.chunk(x.size()[0])\n",
    "    for xi in x:\n",
    "        o = model(xi.squeeze(), *states)\n",
    "        states = o[1]\n",
    "        out += [o[0]]\n",
    "    out = torch.stack(out, dim = 0)\n",
    "    loss = F.binary_cross_entropy(out * w.expand_as(out), t)\n",
    "    loss.backward()\n",
    "    clip_grad_norm(model.parameters(), 10)\n",
    "    optimizer.step()\n",
    "\n",
    "    progbar.add(1, values=[(\"loss\", loss.data.cpu().numpy())])\n",
    "    if int(e + 1) % 500 == 0:\n",
    "        out_np = np.transpose(np.round(out.data.cpu().numpy()), (1, 0, 2))\n",
    "        acc = (out_np == targets)[:, -20:].mean()\n",
    "        print('\\naccuracy ', acc)\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.title('input')\n",
    "        plt.imshow(data[0], cmap='gray')\n",
    "        plt.subplot(132)\n",
    "        plt.title('desired')\n",
    "        plt.imshow(targets[0], cmap='gray')\n",
    "        plt.subplot(133)\n",
    "        plt.title('output')\n",
    "        plt.imshow(out.data.cpu().numpy()[:, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.title('input')\n",
    "plt.imshow(data[0], cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.title('desired')\n",
    "plt.imshow(targets[0], cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.title('output')\n",
    "plt.imshow(out.data.cpu().numpy()[:, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
